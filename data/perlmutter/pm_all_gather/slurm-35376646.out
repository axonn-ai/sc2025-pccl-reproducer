
The following have been reloaded with a version change:
  1) cudatoolkit/12.2 => cudatoolkit/12.4

srun -C gpu -N 4 -n 16 -c 32 --cpu-bind=cores --gpus-per-node=4 ./get_rank.sh python -u benchmark_all_gather.py
PE 0: MPICH CH4 OFI detected 4 NICs/node on host nid001997
PE 0: MPICH CH4 OFI netmod using cxi provider (domain_name=cxi0, src_addr=0x63e3)
PE 0: Selected traffic class: [TC_BEST_EFFORT, 512]
output size = 1 MB
Method = nccl
All-gather bus bw for 16 GPUs is 4.431 GBPS for message output size 1.049 MB
time = 0.222 ms
Method = mpi
All-gather bus bw for 16 GPUs is 3.670 GBPS for message output size 1.049 MB
time = 0.268 ms
Method = hybrid
All-gather bus bw for 16 GPUs is 3.461 GBPS for message output size 1.049 MB
time = 0.284 ms
===============================
output size = 2 MB
Method = nccl
All-gather bus bw for 16 GPUs is 7.784 GBPS for message output size 2.097 MB
time = 0.253 ms
Method = mpi
All-gather bus bw for 16 GPUs is 5.857 GBPS for message output size 2.097 MB
time = 0.336 ms
Method = hybrid
All-gather bus bw for 16 GPUs is 6.630 GBPS for message output size 2.097 MB
time = 0.297 ms
===============================
output size = 4 MB
Method = nccl
All-gather bus bw for 16 GPUs is 11.220 GBPS for message output size 4.194 MB
time = 0.350 ms
Method = mpi
All-gather bus bw for 16 GPUs is 7.923 GBPS for message output size 4.194 MB
time = 0.496 ms
Method = hybrid
All-gather bus bw for 16 GPUs is 11.994 GBPS for message output size 4.194 MB
time = 0.328 ms
===============================
output size = 8 MB
Method = nccl
All-gather bus bw for 16 GPUs is 17.898 GBPS for message output size 8.389 MB
time = 0.439 ms
Method = mpi
All-gather bus bw for 16 GPUs is 9.725 GBPS for message output size 8.389 MB
time = 0.809 ms
Method = hybrid
All-gather bus bw for 16 GPUs is 19.103 GBPS for message output size 8.389 MB
time = 0.412 ms
===============================
output size = 16 MB
Method = nccl
All-gather bus bw for 16 GPUs is 33.804 GBPS for message output size 16.777 MB
time = 0.465 ms
Method = mpi
All-gather bus bw for 16 GPUs is 10.940 GBPS for message output size 16.777 MB
time = 1.438 ms
Method = hybrid
All-gather bus bw for 16 GPUs is 25.263 GBPS for message output size 16.777 MB
time = 0.623 ms
===============================
output size = 32 MB
Method = nccl
All-gather bus bw for 16 GPUs is 46.871 GBPS for message output size 33.554 MB
time = 0.671 ms
Method = mpi
All-gather bus bw for 16 GPUs is 11.398 GBPS for message output size 33.554 MB
time = 2.760 ms
Method = hybrid
All-gather bus bw for 16 GPUs is 31.482 GBPS for message output size 33.554 MB
time = 0.999 ms
===============================
output size = 64 MB
Method = nccl
All-gather bus bw for 16 GPUs is 55.096 GBPS for message output size 67.109 MB
time = 1.142 ms
Method = mpi
All-gather bus bw for 16 GPUs is 19.086 GBPS for message output size 67.109 MB
time = 3.296 ms
Method = hybrid
All-gather bus bw for 16 GPUs is 34.013 GBPS for message output size 67.109 MB
time = 1.850 ms
===============================
output size = 128 MB
Method = nccl
All-gather bus bw for 16 GPUs is 57.561 GBPS for message output size 134.218 MB
time = 2.186 ms
Method = mpi
All-gather bus bw for 16 GPUs is 18.992 GBPS for message output size 134.218 MB
time = 6.625 ms
Method = hybrid
All-gather bus bw for 16 GPUs is 37.105 GBPS for message output size 134.218 MB
time = 3.391 ms
===============================
output size = 256 MB
Method = nccl
All-gather bus bw for 16 GPUs is 56.559 GBPS for message output size 268.435 MB
time = 4.449 ms
Method = mpi
All-gather bus bw for 16 GPUs is 18.326 GBPS for message output size 268.435 MB
time = 13.733 ms
Method = hybrid
All-gather bus bw for 16 GPUs is 37.846 GBPS for message output size 268.435 MB
time = 6.649 ms
===============================
output size = 512 MB
Method = nccl
All-gather bus bw for 16 GPUs is 59.276 GBPS for message output size 536.871 MB
time = 8.491 ms
Method = mpi
All-gather bus bw for 16 GPUs is 18.086 GBPS for message output size 536.871 MB
time = 27.829 ms
Method = hybrid
All-gather bus bw for 16 GPUs is 44.257 GBPS for message output size 536.871 MB
time = 11.373 ms
===============================
output size = 1024 MB
Method = nccl
All-gather bus bw for 16 GPUs is 61.962 GBPS for message output size 1073.742 MB
time = 16.246 ms
Method = mpi
All-gather bus bw for 16 GPUs is 18.189 GBPS for message output size 1073.742 MB
time = 55.344 ms
Method = hybrid
All-gather bus bw for 16 GPUs is 43.837 GBPS for message output size 1073.742 MB
time = 22.963 ms
===============================
{'mpi': [np.float64(3.6697247327013174), np.float64(5.857062538726275), np.float64(7.922528323887306), np.float64(9.725443142281772), np.float64(10.939635369999278), np.float64(11.398146670335851), np.float64(19.085579727411503), np.float64(18.991997008111255), np.float64(18.325596959399775), np.float64(18.085787826950117), np.float64(18.188610342503253)], 'rccl': [np.float64(4.430726664838515), np.float64(7.784408280257257), np.float64(11.220483236722599), np.float64(17.897795556978245), np.float64(33.8037470568297), np.float64(46.871468376425774), np.float64(55.09588152436214), np.float64(57.56067978497143), np.float64(56.5594916571699), np.float64(59.27574060871484), np.float64(61.96205551432922)], 'hybrid': [np.float64(3.4612914531339474), np.float64(6.6296196744449984), np.float64(11.993909381338716), np.float64(19.10299256827318), np.float64(25.263417567760037), np.float64(31.481659629765375), np.float64(34.01257713054486), np.float64(37.10477472519443), np.float64(37.84649505566759), np.float64(44.25692113219975), np.float64(43.83663637647817)]}
